<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">

**Report**
Comparison pipeline:
1. Establish scene in blender.
2. Export Nori.xml and Mitsuba.xml respectively
3. Check the xml files with necessary manual adjustment
4. Align the ply and obj file, ensure they are the same
5. Rendering with both ones and compare the images

# Images as Textures
**modified files:**

- `src/map/texture_map.cpp`


Given an image, we can attach the image onto object, making it looks more realistic without increasing the geometry complexity of mesh.
Specifically, for arbitrary point in 3D space, we could map it into a 2D space point, and query the color of that 2D point from the given image.
The most important thing is to understand the mapping from UV space to XY image space, because UV space is continuous but XY space is discrete (only integer indices has pixel value).
$$
    x = \frac{u}{\frac{1}{width}}=u \times width \\
    y = \frac{v}{\frac{1}{height}}=v \times height
$$

To make XY space become continuous, we could use bilinear interpolation with the values in the four corners, like:
```cpp
// integer part
int x_int = int(std::floor(x));
int y_int = int(std::floor(y));

float x_frac = x - float(x_int);
float y_frac = y - float(y_int);

T interpolated_value =
    (1 - x_frac) * (1 - y_frac) * v00 +
    x_frac * (1 - y_frac) * v10 +
    (1 - x_frac) * y_frac * v01 +
    x_frac * y_frac * v11;
```

Also need to consider the normalization of given uv input (maybe negative, maybe out of range), and need to align the frame
convention (e.g., origin, axis) with the output format of Blender using the checkerboard pattern image on a plane:
```cpp
// map UV in any range to [0, 1]
Point2f normalized_uv;
normalized_uv.x() = uv.x() - std::floor(uv.x());
normalized_uv.y() = uv.y() - std::floor(uv.y());
// flip V to align convention
// UV coord set origin at left bottom
// image coord set origin at left top
normalized_uv.y() = 1.0f - normalized_uv.y();

//change to image space
float x = normalized_uv.x() / m_scale.x() * (m_width - 1);
float y = normalized_uv.y() / m_scale.y() * (m_height - 1);
```
<div class="twentytwenty-container">
    <img src="ref/texture/floor_mitsuba.png" alt="Mitsuba" class="img-responsive">
    <img src="solution/texture/floor_nori.png" alt="Mine" class="img-responsive">
</div>

The scale property could make the image applied on mesh after scaling, like using`(0.5, 1)` would make u direction to densify as 2 times of the original one:
<div class="twentytwenty-container">
    <img src="solution/texture/floor_scale_nori.png" alt="(0.5,1)" class="img-responsive">
    <img src="solution/texture/floor_nori.png" alt="(1,1)" class="img-responsive">
</div>

This feature support `jpg`, `png` and `exr` formats. For `jpg` and `png` formats, I use `stbi` tiny library to process the data, careful with the layout (RGBA, row major).

```cpp
std::cout << "Processing jpg/png file: " << filename << std::endl;
// using stbi library
// 1.RGBA/RGB 2.column first order lay out
int channels;
unsigned char* data = stbi_load(filename.c_str(), &m_width, &m_height, &channels, 3);
if (!data) {
    std::cerr << "Failed to load jpg/png image: " << filename << std::endl;
    return false;
}
m_normal.reserve(m_width * m_height);
for (int y = 0; y < m_height; ++y) {
    for (int x = 0; x < m_width; ++x) {
        //careful with channels!
        float R = data[channels * (y * m_width + x)] / 255.0f;
        float G = data[channels * (y * m_width + x) + 1] / 255.0f;
        float B = data[channels * (y * m_width + x) + 2] / 255.0f;
        m_normal.push_back(Color3f(R, G, B));
    }
}
//release mem
stbi_image_free(data);
```

For `exr` format, I use `bitmap` which is already implemented.
```cpp
std::cout << "Processing EXR file: " << filename << std::endl;
// using bit map
Bitmap bitmap(filename);
m_height = bitmap.rows();
m_width = bitmap.cols();
for (int y = 0; y < m_height; ++y) {
    for (int x = 0; x < m_width; ++x) {
        m_normal.push_back(bitmap(y, x));
    }
}
```
What need to be careful about is the notion of `sRGB` and `linearRGB`, when we do calculating (e.g., integration)
we use `linearRGB`, but the color of image is stored as `sRGB`, so a transformation (gamma correction) after reading the image is necessary.
<div class="twentytwenty-container">
    <img src="solution/texture/sphere_nori.png" alt="toLinear" class="img-responsive">
    <img src="solution/texture/sphere_sRGB_nori.png" alt="sRGB" class="img-responsive">
</div>


## Validation
I used the plugin of Blender to export xml format of Mitsuba and Nori respectively. Then I used Mitsuba and Nori to render the same scene, using path tracing with mis (default parameters, spp is 32) and diffuse bsdf.
<div class="twentytwenty-container">
    <img src="ref/texture/sphere_mitsuba.png" alt="Mitsuba" class="img-responsive">
    <img src="solution/texture/sphere_nori.png" alt="Mine" class="img-responsive">
</div>

We could find the results are almost the same. The minor shadow difference is due to the implementation of path tracer. I suppose it's some different parameters like `max_depth` causing this.

# Normal Mapping
**modified files:**

- `include/nori/bsdf.h`

- `include/nori/mesh.h`

- `src/map/normal_map.cpp`

- `src/diffuse.cpp`

- `src/obj.cpp`

- `src/mesh.cpp`




Like image texture, we could use normal mapping to render more realistic pictures without modifying the real detailed geometry
(just adjusting the shading normal of the hitting point using the queried normal). Almost the same as image texture
(e.g., repeat wrap mode, bilinear interpolation,  map scaling), we could implement the normal version of image texture. What important is the range of normal, we should first
transform the rgb range (maybe out of range for EXR image) into `[0, 1]` using
$$ v' = \frac{v - min(img)}{max(img) - min(img)}$$
then transfer the range from `[0, 1]` into `[-1, 1]`:
$$ n = 2 \times v' - 1$$

```cpp
Color3f normalized_rgb = (rgb - min) / (max - min);
```

And also because the normal should be stored into the normal map directly (without gamma correction), so we don't need to map the value back as well when using:
```cpp
    T getPixel(int x, int y) {
        return m_normal[y * m_width + x];
    }
```
First change bsdf.h interface to allow us query the normal:

```cpp
virtual bool getNormalMap(Texture&ltNormal3f> * &normal_map) const { return false; }
```

For diffuse.cpp, add normalmap object as child:
```cpp
    if (obj->getIdName() == "normalmap") {
        if (m_normalmap) {
            throw NoriException("There is already a normalmap defined!");
        }
        m_normalmap = static_cast&ltTexture&ltNormal3f> *>(obj);
    }
```

To equip with consistent tangent across the surface, we could use UV coords to compute a shading frame instead of just using geometry frame (not consistent in UV space),
        we compute the TBN matrix when loading object in `obj.cpp`:
```cpp
    void compute_pervertex_TBN() {
        // column first for Eigen
        // Initialize m_T and m_B to have the same size as m_V (3, N)
        m_T.resize(3, m_V.cols());
        m_B.resize(3, m_V.cols());

        // Iterate over each face to compute tangents and bitangents
        for (int i = 0; i < m_F.cols(); ++i) {
            // Get vertex indices for the current face
            int idx0 = m_F(0, i);
            int idx1 = m_F(1, i);
            int idx2 = m_F(2, i);

            // Get positions and UVs for the current face
            Vector3f p0 = m_V.col(idx0);
            Vector3f p1 = m_V.col(idx1);
            Vector3f p2 = m_V.col(idx2);

            Vector2f uv0 = m_UV.col(idx0);
            Vector2f uv1 = m_UV.col(idx1);
            Vector2f uv2 = m_UV.col(idx2);

            // Compute edges and delta UVs
            Vector3f edge1 = p1 - p0;
            Vector3f edge2 = p2 - p0;
            Vector2f deltaUV1 = uv1 - uv0;
            Vector2f deltaUV2 = uv2 - uv0;

            // Compute tangent and bitangent for this face
            float r = 1.0f / (deltaUV1.x() * deltaUV2.y() - deltaUV1.y() * deltaUV2.x());
            Vector3f T = r * (deltaUV2.y() * edge1 - deltaUV1.y() * edge2);
            Vector3f B = r * (-deltaUV2.x() * edge1 + deltaUV1.x() * edge2);

            // Accumulate the results to the vertices
            m_T.col(idx0) += T;
            m_T.col(idx1) += T;
            m_T.col(idx2) += T;

            m_B.col(idx0) += B;
            m_B.col(idx1) += B;
            m_B.col(idx2) += B;
        }

        // Normalize accumulated tangents and bitangents
        for (int i = 0; i < m_V.cols(); ++i) {
            m_T.col(i).normalize();
            m_B.col(i).normalize();
        }

        for (int i = 0; i < m_V.cols(); ++i) {
            Vector3f N = m_N.col(i).normalized(); // Per-vertex normal
            Vector3f T = m_T.col(i);
            Vector3f B = m_B.col(i);

            // Re-orthogonalize tangent and compute corrected bitangent
            T = (T - N.dot(T) * N).normalized();
            B = N.cross(T).normalized();

            // Store back the orthogonalized results
            m_T.col(i) = T;
            m_B.col(i) = B;
        }
    }
```
In `mesh.cpp`, at the hit points we need to adjust the normal, and now we have TBN matrix:

```cpp
    if (m_N.size() > 0) {
        Vector3f t = (bary.x() * m_T.col(idx0) +
        bary.y() * m_T.col(idx1) +
        bary.z() * m_T.col(idx2)).normalized();

        Vector3f b = (bary.x() * m_B.col(idx0) +
        bary.y() * m_B.col(idx1) +
        bary.z() * m_B.col(idx2)).normalized();

        Vector3f n = (bary.x() * m_N.col(idx0) +
        bary.y() * m_N.col(idx1) +
        bary.z() * m_N.col(idx2)).normalized();

        its.shFrame = Frame(t, b, n);

    } else {
        its.shFrame = its.geoFrame;
    }

    Texture&ltNormal3f> *normal_map;
    if (its.mesh->getBSDF()->getNormalMap(normal_map)) {
        // using normal map to reformulate shFrame
        Normal3f local_n = normal_map->eval(its.uv).normalized();
        Normal3f world_n = its.shFrame.toWorld(local_n).normalized();
        its.shFrame = Frame(world_n);
        // TBN already under global
        Vector3f s = its.shFrame.s;
        s = (s - world_n.dot(s) * world_n).normalized();
        Vector3f t = world_n.cross(s);
        its.shFrame = Frame(s, t, world_n);
    }
```


## Validation
First, I use a plane on the ground, and place the point light on the left. We could find normal map makes the rendering more realistic even with the same original flat plane.
<div class="twentytwenty-container">
    <img src="solution/normal/floor_wo_normal_nori.png" alt="wo normal map" class="img-responsive">
    <img src="solution/normal/floor_normal_nori.png" alt="w normal map" class="img-responsive">
</div>

To make sure the convention is the same, I also change the place of light to the top, and compared the results with Mitsuba,
            the light instensity is slightly different, please just ignore that (maybe use env map later to validate again TODO):
<div class="twentytwenty-container">
    <img src="ref/normal/floor_normal_mitsuba.png" alt="Mitsuba" class="img-responsive">
    <img src="solution/normal/floor_normal_nori.png" alt="Mine" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="ref/normal/floor_normal_2_mitsuba.png" alt="Mitsuba" class="img-responsive">
    <img src="solution/normal/floor_normal_2_nori.png" alt="Mine" class="img-responsive">
</div>

Then I tested more complicated materials on sphere:

For to using TBN map:
<div class="twentytwenty-container">
    <img src="ref/normal/sphere_normal_mitsuba.png" alt="Mitsuba, w normal map" class="img-responsive">
    <img src="solution/normal/sphere_normal_wo_TBN_nori.png" alt="Mine, w normal map" class="img-responsive">
    <img src="ref/normal/sphere_wo_normal_mitsuba.png" alt="Mitsuba, wo normal map" class="img-responsive">
    <img src="solution/normal/sphere_wo_normal_nori.png" alt="Mine, wo normal map" class="img-responsive">
</div>
After applying the TBN map:
<div class="twentytwenty-container">
    <img src="ref/normal/sphere_normal_mitsuba.png" alt="Mitsuba, w normal map" class="img-responsive">
    <img src="solution/normal/sphere_normal_nori.png" alt="Mine, w normal map" class="img-responsive">
    <img src="ref/normal/sphere_wo_normal_mitsuba.png" alt="Mitsuba, wo normal map" class="img-responsive">
    <img src="solution/normal/sphere_wo_normal_nori.png" alt="Mine, wo normal map" class="img-responsive">
</div>

We could conclude these:
1. For image texture, TBN doesn't influence a lot, due to not transforming the normal from local to global with shading frame.
2. For plane, even without TBN map, normal map could work well with inherent consistent tangent across the surface.
3. For more complex mesh(e.g., sphere), with TBN map, we could get more consistent and realistic reflection.

# Mesh Modelling

# Environment Map

**modified files:**

- `include/nori/emitter.h`

- `src/emitter/envmaplight.cpp`

- `src/path_mis.cpp`

- `src/path_mats.cpp`

Environment map is a method using an exr file to wrap the scene at indefinite distance, every pixel would be mapped into
a single indefinite light source point according to its radiance. Using this method, we don't need to place many lights
manually anymore and can get realistic and consistent lighting. Besides, we could also get background without pure black
colors when not hitting light sources (in any direction we would finally hit a light source point with env map).

We first implement envlight envmaplight relative files, in `emitter.h`, we need to judge if the emitter is envmaplight or not,
and need a counter to validate importance sampling later.
```cpp
virtual bool isEnvMapLight() const { return false; }

virtual void counter() {
    m_counter++;
}
```

Then in `envmaplight.cpp`, we handle env map like other lights, and extends `Emitter` class. The pipeline to handle env map is:
1. read an exr file
2. map the radiance(pixel value) into luminance, then to pdf, preprocess the discrete pdf (proportional to radiance) to make importance sampling available
3. inverse transform sampling to get samples (importance sampling)

we have the listed properties,
```cpp
std::vector&ltColor3f> m_radiance;
std::vector&ltfloat> m_luminance;
//1D discrete
DiscretePDF m_marginal;
//2D discrete
std::vector&ltDiscretePDF> m_conditionals;

// used for adjust env map
Transform m_worldToLight;
Transform m_lightToWorld;

float m_radiance_scale;
int m_width;
int m_height;
std::string m_file;
//for IS validation
// for red marking
Bitmap *m_bitmap_val;
// validation switch
bool m_val;
// samples of red pts
int m_samples;
```
## read
To align with Mitsuba, mirror along each rows:
```cpp
for (int y = 0; y < m_height; ++y) {
    // mirror along width: change right hand frame to left hand frame(align with mitsuba)
    for (int x = m_width - 1; x >= 0; --x) {
        m_radiance.push_back(bitmap(y, x));
        // for validation need to mirror back
        if (m_val)
            m_bitmap_val->coeffRef(y, x) = bitmap(y, m_width - 1 - x);
    }
}
```

## preprocess
What really important is to take care there are actually to mapping, the first is from `uv` to `theta, phi`, the second
is from `theta, phi` to `omega`, here what is stored is the pdf w.r.t `uv`, i.e.:
$$p(u,v) \propto L(\theta, \phi) \times sin(\theta)$$
Not the below cases, because using the below cases, you already omitted the resolution when using $\propto$ without notice,
leading the pdf calculation to be wrong!
$$p(\theta, \phi) \propto L(\theta, \phi) \times sin(\theta)$$
```cpp
// precompute m_radiance with sin(theta) term
m_luminance.resize(m_radiance.size());
for (int y = 0; y < m_height; ++y) {
    // prevent sin(theta) equals 0.f
    float sinTheta = abs(sin((y + 0.5f) * (M_PI / m_height)));
    for (int x = 0; x < m_width; ++x) {
        m_luminance[y * m_width + x] = m_radiance[y * m_width + x].getLuminance() * sinTheta;
    }
}

// preprocess for sampling the env map light
// calculate the CDF struct of marginal p(u)
m_marginal = DiscretePDF (m_height);
for (int y = 0; y < m_height; ++y) {
    float rowSum = 0.0f;
    for (int x = 0; x < m_width; ++x) {
        rowSum += m_luminance[y * m_width + x]; // Sum of all radiance in row `y`
    }
    m_marginal.append(rowSum); // Append the row sum to the marginal PDF
}
m_marginal.normalize(); // Normalize the marginal PDF

// calculate the CDF struct of conditional probability
m_conditionals.resize(m_height);
for (int y = 0; y < m_height; ++y) {
    // p(v|u)
    DiscretePDF conditional(m_width);
    for (int x = 0; x < m_width; ++x) {
        // p(v|u) = p(v, u) / p(u)
        conditional.append(m_luminance[y * m_width + x]); // Append value for each column
    }
    conditional.normalize(); // Normalize the conditional PDF for row `y`
    m_conditionals[y] = conditional; // Store the CDF struct
}
```
## sampling
For sampling, we could directly use the DiscretePDF which's provided already:
```cpp
// fill properties of query record before return
// p is at infinite far place, ignore shadowRay and p
// sample xi uniformly in [0, 1] then get u, v
float xi1 = sample.x();
float xi2 = sample.y();
float p_u = 0.0f, p_v = 0.0f;
// importance sampling
// xy: corresponding grid index
int y = m_marginal.sample(xi1, p_u);
int x = m_conditionals[y].sample(xi2, p_v);
// get corresponding uv domain values [0, 1]
float u = float(y) / m_height;
float v = float(x) / m_width;
// map uv to theta phi domain
float theta = u * M_PI;
float phi = v * 2 * M_PI;
Vector3f wi = sphericalDirection(theta, phi);
lRec.wi = m_lightToWorld * wi;
lRec.shadowRay = Ray3f(lRec.ref, lRec.wi, Epsilon, INFINITY);
lRec.pdf = pdf(lRec);
```
For eval function, just return the corresponding grid value, I just use neareast neighbor:
```cpp
Vector3f w = (m_worldToLight * lRec.wi).normalized();

Point2f res = sphericalCoordinates(w);
float theta = res.x();
float phi = res.y();
float sinTheta = std::sin(theta);
// 0 probability
if (sinTheta < Epsilon) {
    return 0.0f;
}

// u: theta [0, pi]
// v: phi[0, 2 * pi]
// uv in range [0, 1]
float u = theta / M_PI;
float v = phi / (2 * M_PI);
// find the corresponding grid to return value
int y = round(u * (m_height - 1));
int x = round(v * (m_width - 1));

return m_radiance[y * m_width + x] * m_radiance_scale;
```
For pdf calculation, we need to apply to jacobians due to two mapping, if we regard the stored pdf as $p(\theta, \phi)$,
then the resolution is missing, causing resolution influencing final joint pdf (can't cancel out):
$$
    p(\omega) = \frac{p(\theta, \phi)}{sin(\theta)} = \frac{p(u, v)}{sin(\theta) \times 2 \pi ^2}
$$
```cpp
Vector3f w = (m_worldToLight * lRec.wi).normalized();
// u:theta v:phi
Point2f res = sphericalCoordinates(w);
float theta = res.x();
float phi = res.y();
float sinTheta = std::sin(theta);
// 0 probability
if (sinTheta < Epsilon) {
    return 0.0f;
}

float u = theta / M_PI;
float v = phi / (2 * M_PI);

// map to corresponding grid
int y = round(u * (m_height - 1));
int x = round(v * (m_width - 1));

//p(u)
float p_u = m_marginal[y];
//p(v|u)
float p_v = m_conditionals.at(y)[x];

// joint distribution for p(u, v)
float p_joint = p_u * p_v;
return p_joint * (m_width * m_height) / (sinTheta * 2 * M_PI * M_PI);
```

## Validation
First I used a low resolution exr file (1000x500), we could find using nearest neighbor instead of interpolation,
making the env map very blurry compared to Mitsuba.
<div class="twentytwenty-container">
    <img src="ref/envmap/wells6_mitsuba.png" alt="Mitsuba" class="img-responsive">
    <img src="solution/envmap/wells6_env_nori.png" alt="Mine" class="img-responsive">
</div>

Then I used a high resolution exr file (8192x4096), this time, the results nearly the same, but Mitsuba seems got some
black artifacts with unknown reasons.
<div class="twentytwenty-container">
    <img src="ref/envmap/starry_night_mitsuba.png" alt="Mitsuba" class="img-responsive">
    <img src="solution/envmap/starry_night_nori.png" alt="Mine" class="img-responsive">
</div>

To make sure we really used importance sampling, we could place red marks on the place we sampled out, the results show we
indeed sampled the points accroding to the radiance (we alaways got more samples from light):
<div class="twentytwenty-container">
    <img src="solution/envmap/IS_env_5000.png" alt="5000 samples" class="img-responsive">
    <img src="solution/envmap/IS_env_200000.png" alt="200000 samples" class="img-responsive">
</div>

I also tested the radiance_scale function:
<div class="twentytwenty-container">
    <img src="solution/envmap/material_env_high_res_nori.png" alt="1.0 radiance scale" class="img-responsive">
    <img src="solution/envmap/material_env_high_res_0.1_scale_nori.png" alt="0.1 radiance scale" class="img-responsive">
</div>

More validations on different materials (dielectrics, pure diffuse, textured diffuse, mirror) and object (analytical and mesh sphere),
all the results match with Mitsuba:
<div class="twentytwenty-container">
    <img src="ref/envmap/material_ana_env_mitsuba.png" alt="Mitsuba, analytical sphere" class="img-responsive">
    <img src="solution/envmap/material_ana_env_nori.png" alt="Mine, analytical sphere" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="ref/envmap/material_env_high_res_mitsuba.png" alt="Mitsuba, mesh sphere" class="img-responsive">
    <img src="solution/envmap/material_env_high_res_nori.png" alt="Mine, mesh sphere" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="ref/envmap/material_env_low_res_mitsuba.png" alt="Mitsuba, low res exr" class="img-responsive">
    <img src="solution/envmap/material_env_low_res_nori.png" alt="Mine, low res exr" class="img-responsive">
</div>



# Homogeneous Participating Media

# Disney BSDF


<!-- Bootstrap core CSS and JavaScript -->

<link href="../resources/offcanvas.css" rel="stylesheet">
<link href="../resources/twentytwenty.css" rel="stylesheet" type="text/css" />

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="../resources/bootstrap.min.js"></script>
<script src="../resources/jquery.event.move.js"></script>
<script src="../resources/jquery.twentytwenty.fix.js"></script>

<!-- Markdeep: -->
<script>var markdeepOptions = {onLoad: function() {$(".twentytwenty-container").twentytwenty({default_offset_pct: 0.5, move_slider_on_hover: true, placeholder: '../resources/nori.png'});},tocStyle:'none'};</script>
<script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>
<script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>