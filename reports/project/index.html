<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<a href="#Images as Textures">Images as Textures</a>

<a href="#Normal Mapping">Normal Mapping</a>

<a href="#Mesh Modelling">Mesh Modelling</a>

<a href="#Environment Map">Environment Map</a>

<a href="#Homogeneous Participating Media">Homogeneous Participating Media</a>

<a href="#Disney BSDF">Disney BSDF</a>

**Report**
Comparison pipeline:
1. Establish scene in blender.
2. Export Nori.xml and Mitsuba.xml respectively
3. Check the xml files with necessary manual adjustment
4. Align the ply and obj file, ensure they are the same
5. Rendering with both ones and compare the images

<h1 id="Images as Textures">Images as Textures</h1>
**modified files:**

- `src/map/texture_map.cpp`


Given an image, we can attach the image onto object, making it looks more realistic without increasing the geometry complexity of mesh.
Specifically, for arbitrary point in 3D space, we could map it into a 2D space point, and query the color of that 2D point from the given image.
The most important thing is to understand the mapping from UV space to XY image space, because UV space is continuous but XY space is discrete (only integer indices has pixel value).
$$
    x = \frac{u}{\frac{1}{width}}=u \times width \\
    y = \frac{v}{\frac{1}{height}}=v \times height
$$

To make XY space become continuous, we could use bilinear interpolation with the values in the four corners, like:
```cpp
// integer part
int x_int = int(std::floor(x));
int y_int = int(std::floor(y));

float x_frac = x - float(x_int);
float y_frac = y - float(y_int);

T interpolated_value =
    (1 - x_frac) * (1 - y_frac) * v00 +
    x_frac * (1 - y_frac) * v10 +
    (1 - x_frac) * y_frac * v01 +
    x_frac * y_frac * v11;
```

Also need to consider the normalization of given uv input (maybe negative, maybe out of range), and need to align the frame
convention (e.g., origin, axis) with the output format of Blender using the checkerboard pattern image on a plane:
```cpp
// map UV in any range to [0, 1]
Point2f normalized_uv;
normalized_uv.x() = uv.x() - std::floor(uv.x());
normalized_uv.y() = uv.y() - std::floor(uv.y());
// flip V to align convention
// UV coord set origin at left bottom
// image coord set origin at left top
normalized_uv.y() = 1.0f - normalized_uv.y();

//change to image space
float x = normalized_uv.x() / m_scale.x() * (m_width - 1);
float y = normalized_uv.y() / m_scale.y() * (m_height - 1);
```
<div class="twentytwenty-container">
    <img src="ref/texture/floor_mitsuba.png" alt="Mitsuba" class="img-responsive">
    <img src="solution/texture/floor_nori.png" alt="Mine" class="img-responsive">
</div>

The scale property could make the image applied on mesh after scaling, like using`(0.5, 1)` would make u direction to densify as 2 times of the original one:
<div class="twentytwenty-container">
    <img src="solution/texture/floor_scale_nori.png" alt="(0.5,1)" class="img-responsive">
    <img src="solution/texture/floor_nori.png" alt="(1,1)" class="img-responsive">
</div>

This feature support `jpg`, `png` and `exr` formats. For `jpg` and `png` formats, I use `stbi` tiny library to process the data, careful with the layout (RGBA, row major).

```cpp
std::cout << "Processing jpg/png file: " << filename << std::endl;
// using stbi library
// 1.RGBA/RGB 2.column first order lay out
int channels;
unsigned char* data = stbi_load(filename.c_str(), &m_width, &m_height, &channels, 3);
if (!data) {
    std::cerr << "Failed to load jpg/png image: " << filename << std::endl;
    return false;
}
m_normal.reserve(m_width * m_height);
for (int y = 0; y < m_height; ++y) {
    for (int x = 0; x < m_width; ++x) {
        //careful with channels!
        float R = data[channels * (y * m_width + x)] / 255.0f;
        float G = data[channels * (y * m_width + x) + 1] / 255.0f;
        float B = data[channels * (y * m_width + x) + 2] / 255.0f;
        m_normal.push_back(Color3f(R, G, B));
    }
}
//release mem
stbi_image_free(data);
```

For `exr` format, I use `bitmap` which is already implemented.
```cpp
std::cout << "Processing EXR file: " << filename << std::endl;
// using bit map
Bitmap bitmap(filename);
m_height = bitmap.rows();
m_width = bitmap.cols();
for (int y = 0; y < m_height; ++y) {
    for (int x = 0; x < m_width; ++x) {
        m_normal.push_back(bitmap(y, x));
    }
}
```
What need to be careful about is the notion of `sRGB` and `linearRGB`, when we do calculating (e.g., integration)
we use `linearRGB`, but the color of image is stored as `sRGB`, so a transformation (gamma correction) after reading the image is necessary.
<div class="twentytwenty-container">
    <img src="solution/texture/sphere_nori.png" alt="toLinear" class="img-responsive">
    <img src="solution/texture/sphere_sRGB_nori.png" alt="sRGB" class="img-responsive">
</div>


## Validation
I used the plugin of Blender to export xml format of Mitsuba and Nori respectively. Then I used Mitsuba and Nori to render the same scene, using path tracing with mis (default parameters, spp is 32) and diffuse bsdf.
<div class="twentytwenty-container">
    <img src="ref/texture/sphere_mitsuba.png" alt="Mitsuba" class="img-responsive">
    <img src="solution/texture/sphere_nori.png" alt="Mine" class="img-responsive">
</div>

We could find the results are almost the same. The minor shadow difference is due to the implementation of path tracer. I suppose it's some different parameters like `max_depth` causing this.

<h1 id="Normal Mapping">Normal Mapping</h1>
**modified files:**

- `include/nori/bsdf.h`

- `include/nori/mesh.h`

- `src/map/normal_map.cpp`

- `src/diffuse.cpp`

- `src/obj.cpp`

- `src/mesh.cpp`




Like image texture, we could use normal mapping to render more realistic pictures without modifying the real detailed geometry
(just adjusting the shading normal of the hitting point using the queried normal). Almost the same as image texture
(e.g., repeat wrap mode, bilinear interpolation,  map scaling), we could implement the normal version of image texture. What important is the range of normal, we should first
transform the rgb range (maybe out of range for EXR image) into `[0, 1]` using
$$ v' = \frac{v - min(img)}{max(img) - min(img)}$$
then transfer the range from `[0, 1]` into `[-1, 1]`:
$$ n = 2 \times v' - 1$$

```cpp
Color3f normalized_rgb = (rgb - min) / (max - min);
```

And also because the normal should be stored into the normal map directly (without gamma correction), so we don't need to map the value back as well when using:
```cpp
    T getPixel(int x, int y) {
        return m_normal[y * m_width + x];
    }
```
First change bsdf.h interface to allow us query the normal:

```cpp
virtual bool getNormalMap(Texture&ltNormal3f> * &normal_map) const { return false; }
```

For diffuse.cpp, add normalmap object as child:
```cpp
    if (obj->getIdName() == "normalmap") {
        if (m_normalmap) {
            throw NoriException("There is already a normalmap defined!");
        }
        m_normalmap = static_cast&ltTexture&ltNormal3f> *>(obj);
    }
```

To equip with consistent tangent across the surface, we could use UV coords to compute a shading frame instead of just using geometry frame (not consistent in UV space),
        we compute the TBN matrix when loading object in `obj.cpp`:
```cpp
    void compute_pervertex_TBN() {
        // column first for Eigen
        // Initialize m_T and m_B to have the same size as m_V (3, N)
        m_T.resize(3, m_V.cols());
        m_B.resize(3, m_V.cols());

        // Iterate over each face to compute tangents and bitangents
        for (int i = 0; i < m_F.cols(); ++i) {
            // Get vertex indices for the current face
            int idx0 = m_F(0, i);
            int idx1 = m_F(1, i);
            int idx2 = m_F(2, i);

            // Get positions and UVs for the current face
            Vector3f p0 = m_V.col(idx0);
            Vector3f p1 = m_V.col(idx1);
            Vector3f p2 = m_V.col(idx2);

            Vector2f uv0 = m_UV.col(idx0);
            Vector2f uv1 = m_UV.col(idx1);
            Vector2f uv2 = m_UV.col(idx2);

            // Compute edges and delta UVs
            Vector3f edge1 = p1 - p0;
            Vector3f edge2 = p2 - p0;
            Vector2f deltaUV1 = uv1 - uv0;
            Vector2f deltaUV2 = uv2 - uv0;

            // Compute tangent and bitangent for this face
            float r = 1.0f / (deltaUV1.x() * deltaUV2.y() - deltaUV1.y() * deltaUV2.x());
            Vector3f T = r * (deltaUV2.y() * edge1 - deltaUV1.y() * edge2);
            Vector3f B = r * (-deltaUV2.x() * edge1 + deltaUV1.x() * edge2);

            // Accumulate the results to the vertices
            m_T.col(idx0) += T;
            m_T.col(idx1) += T;
            m_T.col(idx2) += T;

            m_B.col(idx0) += B;
            m_B.col(idx1) += B;
            m_B.col(idx2) += B;
        }

        // Normalize accumulated tangents and bitangents
        for (int i = 0; i < m_V.cols(); ++i) {
            m_T.col(i).normalize();
            m_B.col(i).normalize();
        }

        for (int i = 0; i < m_V.cols(); ++i) {
            Vector3f N = m_N.col(i).normalized(); // Per-vertex normal
            Vector3f T = m_T.col(i);
            Vector3f B = m_B.col(i);

            // Re-orthogonalize tangent and compute corrected bitangent
            T = (T - N.dot(T) * N).normalized();
            B = N.cross(T).normalized();

            // Store back the orthogonalized results
            m_T.col(i) = T;
            m_B.col(i) = B;
        }
    }
```
In `mesh.cpp`, at the hit points we need to adjust the normal, and now we have TBN matrix:

```cpp
    if (m_N.size() > 0) {
        Vector3f t = (bary.x() * m_T.col(idx0) +
        bary.y() * m_T.col(idx1) +
        bary.z() * m_T.col(idx2)).normalized();

        Vector3f b = (bary.x() * m_B.col(idx0) +
        bary.y() * m_B.col(idx1) +
        bary.z() * m_B.col(idx2)).normalized();

        Vector3f n = (bary.x() * m_N.col(idx0) +
        bary.y() * m_N.col(idx1) +
        bary.z() * m_N.col(idx2)).normalized();

        its.shFrame = Frame(t, b, n);

    } else {
        its.shFrame = its.geoFrame;
    }

    Texture&ltNormal3f> *normal_map;
    if (its.mesh->getBSDF()->getNormalMap(normal_map)) {
        // using normal map to reformulate shFrame
        Normal3f local_n = normal_map->eval(its.uv).normalized();
        Normal3f world_n = its.shFrame.toWorld(local_n).normalized();
        its.shFrame = Frame(world_n);
        // TBN already under global
        Vector3f s = its.shFrame.s;
        s = (s - world_n.dot(s) * world_n).normalized();
        Vector3f t = world_n.cross(s);
        its.shFrame = Frame(s, t, world_n);
    }
```


## Validation
First, I use a plane on the ground, and place the point light on the left. We could find normal map makes the rendering more realistic even with the same original flat plane.
<div class="twentytwenty-container">
    <img src="solution/normal/floor_wo_normal_nori.png" alt="wo normal map" class="img-responsive">
    <img src="solution/normal/floor_normal_nori.png" alt="w normal map" class="img-responsive">
</div>

To make sure the convention is the same, I also change the place of light to the top, and compared the results with Mitsuba,
            the light instensity is slightly different, please just ignore that (maybe use env map later to validate again TODO):
<div class="twentytwenty-container">
    <img src="ref/normal/floor_normal_mitsuba.png" alt="Mitsuba" class="img-responsive">
    <img src="solution/normal/floor_normal_nori.png" alt="Mine" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="ref/normal/floor_normal_2_mitsuba.png" alt="Mitsuba" class="img-responsive">
    <img src="solution/normal/floor_normal_2_nori.png" alt="Mine" class="img-responsive">
</div>

Then I tested more complicated materials on sphere:

For to using TBN map:
<div class="twentytwenty-container">
    <img src="ref/normal/sphere_normal_mitsuba.png" alt="Mitsuba, w normal map" class="img-responsive">
    <img src="solution/normal/sphere_normal_wo_TBN_nori.png" alt="Mine, w normal map" class="img-responsive">
    <img src="ref/normal/sphere_wo_normal_mitsuba.png" alt="Mitsuba, wo normal map" class="img-responsive">
    <img src="solution/normal/sphere_wo_normal_nori.png" alt="Mine, wo normal map" class="img-responsive">
</div>
After applying the TBN map:
<div class="twentytwenty-container">
    <img src="ref/normal/sphere_normal_mitsuba.png" alt="Mitsuba, w normal map" class="img-responsive">
    <img src="solution/normal/sphere_normal_nori.png" alt="Mine, w normal map" class="img-responsive">
    <img src="ref/normal/sphere_wo_normal_mitsuba.png" alt="Mitsuba, wo normal map" class="img-responsive">
    <img src="solution/normal/sphere_wo_normal_nori.png" alt="Mine, wo normal map" class="img-responsive">
</div>

We could conclude these:
1. For image texture, TBN doesn't influence a lot, due to not transforming the normal from local to global with shading frame.
2. For plane, even without TBN map, normal map could work well with inherent consistent tangent across the surface.
3. For more complex mesh(e.g., sphere), with TBN map, we could get more consistent and realistic reflection.

<h1 id="Mesh Modelling">Mesh Modelling</h1>

In this part, I'll prepare all the meshes and textures for the final scene, including the water, Rennala and moon mesh. I modelled the water mesh,
sculpted the Rennala to specific pose by myself, and used the moon mesh from others. Next I'll mainly introduce the part done by myself.

## Water Mesh
I followed the <a href="https://www.bilibili.com/video/BV1qs4y177Pc/?spm_id_from=333.337.search-card.all.click&vd_source=a9063a720abf17fbbf6c9b9f77914e66" target="_blank">tutorial</a> to model the water mesh by myself:

First, I create a plane, and attach an ocean physics modifier, tuning the parameters to find a good water surface.
<div class="twentytwenty-container">
    <img src="solution/mesh/Water1.png" alt="Mitsuba, w normal map" clcreate ass="img-responsive">
</div>
Then select the edges and translate along Z axis, close the mesh to get the initial water cube.
<div class="twentytwenty-container">
    <img src="solution/mesh/Water2.png" alt="Mitsuba, w normal map" class="img-responsive">
</div>
Finally, use another cube to do intersect boolean operation to get the final water cube.
<div class="twentytwenty-container">
    <img src="solution/mesh/Water3.png" alt="Mitsuba, w normal map" class="img-responsive">
</div>
For water material, I'm going to use the dielectrics bsdf with a new color properties added, to make the ocean color deeper.

## Rennala Mesh
Long story short, it's very hard to get the high detailed game character mesh from online resource market (of course),
so I just followed the <a href="https://www.bilibili.com/video/BV1N4421c7R8/?spm_id_from=333.337.search-card.all.click&vd_source=a9063a720abf17fbbf6c9b9f77914e66" target="_blank">tutorial</a> to hack the assets in the game packages,
and extract the mesh and texture, which is possible theoretically due to soul game's shared old engine. But even unpacked the game assets, its format is engine specific (<a href="https://www.bilibili.com/video/BV1N4421c7R8/?spm_id_from=333.337.search-card.all.click&vd_source=a9063a720abf17fbbf6c9b9f77914e66" target="_blank">havok</a>), and then
need some tools to recover them back in normal form (obj mesh, png texture) as much as possible, so that our renderer could load them.
In one word, our task is to import the mesh, and attach the textures in Blender.
But due to complicated reasons during unpacking, some UV map of specific parts are collapsed, making texture mapping impossible.
However, luckily for Rennala, there are two different characters mesh wish different size, and they can contribute to a complete one via scaling.
Also there are some artifacts for some textures (missing corner due to hacking), and there's no method to handle this. Finally, I put mesh and texture together in Blender:
<div class="twentytwenty-container">
    <img src="solution/mesh/Rennala1.png" alt="Init mesh in Blender" class="img-responsive">
</div>
Obviously, that's not the pose we want, and also the clothes texture is wrong, because the pattern appears on the wrong place.
I first used GIMP to move the pattern to the correct place referring the UV map:
<div class="twentytwenty-container">
    <img src="solution/mesh/Rennala2.png" alt="Moving pattern via GIMP" class="img-responsive">
</div>
Second, I need to do skeleton binding to change the pose of character, I used <a href="https://blendermarket.com/products/auto-rig-pro" target="_blank">ARP</a>  plugin in Blender to create general skeleton:
<div class="twentytwenty-container">
    <img src="solution/mesh/Rennala3.png" alt="Creating Skeleton" class="img-responsive">
</div>
<div class="twentytwenty-container">
    <img src="solution/mesh/Rennala4.png" alt="Creating Skeleton" class="img-responsive">
</div>
Then generate the rig:
<div class="twentytwenty-container">
    <img src="solution/mesh/Rennala5.png" alt="Creating Rig" class="img-responsive">
</div>
And as the sleeves and skirts are super long, and the mesh is very high detailed, the automatics skeleton generation is sometimes wrong, so I adjust the weight painting by hands:
<div class="twentytwenty-container">
    <img src="solution/mesh/Rennala6.png" alt="Weight Painting" class="img-responsive">
</div>

Last, because I only need a specific pose, I used the sculpt mode in Blender to sculpt the sleeves to make them more realistic and avoid the penetration:
<div class="twentytwenty-container">
    <img src="solution/mesh/Rennala7.png" alt="Before Sculpting" class="img-responsive">
    <img src="solution/mesh/Rennala8.png" alt="After Sculpting" class="img-responsive">
</div>

Finally, put everything together:
<div class="twentytwenty-container">
    <img src="solution/mesh/scene.png" alt="Scene" class="img-responsive">
</div>

<h1 id="Environment Map">Environment Map</h1>

**modified files:**

- `include/nori/emitter.h`

- `src/emitter/envmaplight.cpp`

- `src/path_mis.cpp`

- `src/path_mats.cpp`

Environment map is a method using an exr file to wrap the scene at indefinite distance, every pixel would be mapped into
a single indefinite light source point according to its radiance. Using this method, we don't need to place many lights
manually anymore and can get realistic and consistent lighting. Besides, we could also get background without pure black
colors when not hitting light sources (in any direction we would finally hit a light source point with env map).

I mainly followed the <a href="https://web.cs.wpi.edu/~emmanuel/courses/cs563/S07/projects/envsample.pdf" target="_blank">paper</a>.
We first implement envlight envmaplight relative files, in `emitter.h`, we need to judge if the emitter is envmaplight or not,
and need a counter to validate importance sampling later.
```cpp
virtual bool isEnvMapLight() const { return false; }

virtual void counter() {
    m_counter++;
}
```

Then in `envmaplight.cpp`, we handle env map like other lights, and extends `Emitter` class. The pipeline to handle env map is:
1. read an exr file
2. map the radiance(pixel value) into luminance, then to pdf, preprocess the discrete pdf (proportional to radiance) to make importance sampling available
3. inverse transform sampling to get samples (importance sampling)

we have the listed properties,
```cpp
std::vector&ltColor3f> m_radiance;
std::vector&ltfloat> m_luminance;
//1D discrete
DiscretePDF m_marginal;
//2D discrete
std::vector&ltDiscretePDF> m_conditionals;

// used for adjust env map
Transform m_worldToLight;
Transform m_lightToWorld;

float m_radiance_scale;
int m_width;
int m_height;
std::string m_file;
//for IS validation
// for red marking
Bitmap *m_bitmap_val;
// validation switch
bool m_val;
// samples of red pts
int m_samples;
```
## read
To align with Mitsuba, mirror along each rows:
```cpp
for (int y = 0; y < m_height; ++y) {
    // mirror along width: change right hand frame to left hand frame(align with mitsuba)
    for (int x = m_width - 1; x >= 0; --x) {
        m_radiance.push_back(bitmap(y, x));
        // for validation need to mirror back
        if (m_val)
            m_bitmap_val->coeffRef(y, x) = bitmap(y, m_width - 1 - x);
    }
}
```

## preprocess
What really important is to take care there are actually to mapping, the first is from `uv` to `theta, phi`, the second
is from `theta, phi` to `omega`, here what is stored is the pdf w.r.t `uv`, i.e.:
$$p(u,v) \propto L(\theta, \phi) \times sin(\theta)$$
Not the below cases, because using the below cases, you already omitted the resolution when using $\propto$ without notice,
leading the pdf calculation to be wrong!
$$p(\theta, \phi) \propto L(\theta, \phi) \times sin(\theta)$$
```cpp
// precompute m_radiance with sin(theta) term
m_luminance.resize(m_radiance.size());
for (int y = 0; y < m_height; ++y) {
    // prevent sin(theta) equals 0.f
    float sinTheta = abs(sin((y + 0.5f) * (M_PI / m_height)));
    for (int x = 0; x < m_width; ++x) {
        m_luminance[y * m_width + x] = m_radiance[y * m_width + x].getLuminance() * sinTheta;
    }
}

// preprocess for sampling the env map light
// calculate the CDF struct of marginal p(u)
m_marginal = DiscretePDF (m_height);
for (int y = 0; y < m_height; ++y) {
    float rowSum = 0.0f;
    for (int x = 0; x < m_width; ++x) {
        rowSum += m_luminance[y * m_width + x]; // Sum of all radiance in row `y`
    }
    m_marginal.append(rowSum); // Append the row sum to the marginal PDF
}
m_marginal.normalize(); // Normalize the marginal PDF

// calculate the CDF struct of conditional probability
m_conditionals.resize(m_height);
for (int y = 0; y < m_height; ++y) {
    // p(v|u)
    DiscretePDF conditional(m_width);
    for (int x = 0; x < m_width; ++x) {
        // p(v|u) = p(v, u) / p(u)
        conditional.append(m_luminance[y * m_width + x]); // Append value for each column
    }
    conditional.normalize(); // Normalize the conditional PDF for row `y`
    m_conditionals[y] = conditional; // Store the CDF struct
}
```
## sampling
For sampling, we could directly use the DiscretePDF which's provided already:
```cpp
// fill properties of query record before return
// p is at infinite far place, ignore shadowRay and p
// sample xi uniformly in [0, 1] then get u, v
float xi1 = sample.x();
float xi2 = sample.y();
float p_u = 0.0f, p_v = 0.0f;
// importance sampling
// xy: corresponding grid index
int y = m_marginal.sample(xi1, p_u);
int x = m_conditionals[y].sample(xi2, p_v);
// get corresponding uv domain values [0, 1]
float u = float(y) / m_height;
float v = float(x) / m_width;
// map uv to theta phi domain
float theta = u * M_PI;
float phi = v * 2 * M_PI;
Vector3f wi = sphericalDirection(theta, phi);
lRec.wi = m_lightToWorld * wi;
lRec.shadowRay = Ray3f(lRec.ref, lRec.wi, Epsilon, INFINITY);
lRec.pdf = pdf(lRec);
```
For eval function, just return the corresponding grid value, I just use neareast neighbor:
```cpp
Vector3f w = (m_worldToLight * lRec.wi).normalized();

Point2f res = sphericalCoordinates(w);
float theta = res.x();
float phi = res.y();
float sinTheta = std::sin(theta);
// 0 probability
if (sinTheta < Epsilon) {
    return 0.0f;
}

// u: theta [0, pi]
// v: phi[0, 2 * pi]
// uv in range [0, 1]
float u = theta / M_PI;
float v = phi / (2 * M_PI);
// find the corresponding grid to return value
int y = round(u * (m_height - 1));
int x = round(v * (m_width - 1));

return m_radiance[y * m_width + x] * m_radiance_scale;
```
For pdf calculation, we need to apply to jacobians due to two mapping, if we regard the stored pdf as $p(\theta, \phi)$,
then the resolution is missing, causing resolution influencing final joint pdf (can't cancel out):
$$
    p(\omega) = \frac{p(\theta, \phi)}{sin(\theta)} = \frac{p(u, v)}{sin(\theta) \times 2 \pi ^2}
$$
```cpp
Vector3f w = (m_worldToLight * lRec.wi).normalized();
// u:theta v:phi
Point2f res = sphericalCoordinates(w);
float theta = res.x();
float phi = res.y();
float sinTheta = std::sin(theta);
// 0 probability
if (sinTheta < Epsilon) {
    return 0.0f;
}

float u = theta / M_PI;
float v = phi / (2 * M_PI);

// map to corresponding grid
int y = round(u * (m_height - 1));
int x = round(v * (m_width - 1));

//p(u)
float p_u = m_marginal[y];
//p(v|u)
float p_v = m_conditionals.at(y)[x];

// joint distribution for p(u, v)
float p_joint = p_u * p_v;
return p_joint * (m_width * m_height) / (sinTheta * 2 * M_PI * M_PI);
```

## Validation
First I used a low resolution exr file (1000x500), we could find using nearest neighbor instead of interpolation,
making the env map very blurry compared to Mitsuba.
<div class="twentytwenty-container">
    <img src="ref/envmap/wells6_mitsuba.png" alt="Mitsuba" class="img-responsive">
    <img src="solution/envmap/wells6_env_nori.png" alt="Mine" class="img-responsive">
</div>

Then I used a high resolution exr file (8192x4096), this time, the results nearly the same, but Mitsuba seems got some
black artifacts with unknown reasons.
<div class="twentytwenty-container">
    <img src="ref/envmap/starry_night_mitsuba.png" alt="Mitsuba" class="img-responsive">
    <img src="solution/envmap/starry_night_nori.png" alt="Mine" class="img-responsive">
</div>

To make sure we really used importance sampling, we could place red marks on the place we sampled out, the results show we
indeed sampled the points accroding to the radiance (we alaways got more samples from light):
<div class="twentytwenty-container">
    <img src="solution/envmap/IS_env_5000.png" alt="5000 samples" class="img-responsive">
    <img src="solution/envmap/IS_env_200000.png" alt="200000 samples" class="img-responsive">
</div>

I also tested the radiance_scale function:
<div class="twentytwenty-container">
    <img src="solution/envmap/material_env_high_res_nori.png" alt="1.0 radiance scale" class="img-responsive">
    <img src="solution/envmap/material_env_high_res_0.1_scale_nori.png" alt="0.1 radiance scale" class="img-responsive">
</div>

More validations on different materials (dielectrics, pure diffuse, textured diffuse, mirror) and object (analytical and mesh sphere),
all the results match with Mitsuba:
<div class="twentytwenty-container">
    <img src="ref/envmap/material_ana_env_mitsuba.png" alt="Mitsuba, analytical sphere" class="img-responsive">
    <img src="solution/envmap/material_ana_env_nori.png" alt="Mine, analytical sphere" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="ref/envmap/material_env_high_res_mitsuba.png" alt="Mitsuba, mesh sphere" class="img-responsive">
    <img src="solution/envmap/material_env_high_res_nori.png" alt="Mine, mesh sphere" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="ref/envmap/material_env_low_res_mitsuba.png" alt="Mitsuba, low res exr" class="img-responsive">
    <img src="solution/envmap/material_env_low_res_nori.png" alt="Mine, low res exr" class="img-responsive">
</div>



<h1 id="Homogeneous Participating Media">Homogeneous Participating Media</h1>
**modified files:**

- `include/nori/interaction.h`

- `include/nori/medium.h`

- `include/nori/ray.h`

- `include/nori/shape.h`

- `src/medium/homogeneous.cpp`

- `src/medium/medium.cpp`

- `src/medium/vol_path_mats.cpp`

- `src/medium/vol_path_mis.cpp`

- `src/bsdf/null.cpp`

- `src/shape.cpp`

- `src/common.cpp`

This feature is about volumetric rendering, where the ray interact with the medium, e.g., absorption, scattering, emission. I implemented the
homogeneous participating media (constant $\sigma_s$, $\sigma_a$) and volumetric path tracer with both material sampling and naive multi-strategy importance sampling (only do mis
outside the volume), following this <a href="https://graphics.pixar.com/library/ProductionVolumeRendering/paper.pdf" target="_blank">paper</a>, and <a href="https://graphics.pixar.com/library/ProductionVolumeRendering/paper.pdf" target="_blank">pbrt's</a> code.
I create a new class `medium` (phase function attached inside), the medium itself is attached to a shape and ray, and a new class `homogeneous` extending `medium`,
and a new class `interaction` extending `intersection`, which has phase function attached inside for more convenient scattering integration (similar to integration with brdf at surface),
For volumetric path tracer, I keep everything the same when outside the volumes, only change the radiance contribution when inside the volumes, and use mis for outside like normal `path_mis`.
Besides, when inside the volumes I didn't do emitter sampling, only when the ray tracing out the volumes can it get contribution from emitters (if hits).
For test convenience, I also create a `null` bsdf class (trivial bsdf), to simulate ray go through the shape bounding directly (otherwise it will be detected as hitting a scene at boundary), so that I can
limit the participating media inside a shape.

## Medium
I mainly follow the pbrt for code. `Medium.h` includes the superclass `Medium`, all kinds of medium has to implement `Tr`
for evaluate transmittance given a ray(has length record inside). `sample` to decide how far to go given a ray, the results will
be stored in the `interaction` class, then use it to do phase function sampling (illustrated later).


```cpp
virtual Color3f Tr(const Ray3f &ray) const = 0;

virtual Color3f sample(const Ray3f &ray, Sampler *sampler,
Interaction &mi) const = 0;
```

For detailed class of medium, I implemented `homogeneous` following pbrt code:

```cpp
Color3f Tr(const Ray3f &ray) const override {
    return Exp(-sigma_t * std::min(ray.maxt * ray.d.norm(), std::numeric_limits&ltfloat>::max()));
}

Color3f sample(const Ray3f &ray, Sampler *sampler, Interaction &mi) const override {
    // Sample a channel and distance along the ray
    int channel = std::min((int)(sampler->next1D() * 3), 2);
    float dist = -std::log(1 - sampler->next1D()) / sigma_t[channel];
    float t = std::min(dist / ray.d.norm(), ray.maxt);
    bool sampledMedium = t < ray.maxt;
    if (sampledMedium) {
        mi = Interaction((ray)(t), ray.reverse().d, m_phaseFunction);
    }

    // Compute the transmittance and sampling density
    Color3f Tr = Exp(-sigma_t * std::min(t, std::numeric_limits&ltfloat>::max()) * ray.d.norm());

    // Return weighting factor for scattering from homogeneous medium
    Color3f density = sampledMedium ? (sigma_t * Tr) : Tr;
    float pdf = 0;
    for (int i = 0; i < 3; ++i) pdf += density[i];
    pdf *= 1.f / 3.f;
    if (pdf == 0.f) {
    // CHECK(Tr.IsBlack());
        if (!Tr.isZero(SEpsilon)) {
            throw NoriException("Tr is not Zero! when pdf is 0.f");
        }
    pdf = 1.f;
    }

    if (sampledMedium) {
        return Tr * sigma_s / pdf;
    } else {
        return Tr / pdf;
    }
}
```

## PhaseFunction
I follow the pbrt, put the phase function and phase function record (like bRec for integration) inside `Medium.h` as well.
Phase function record is almost the same as bsdf record except I removed some useless properties.

For the superclass of all kinds of phase function, it needs `sample` method for MC integration, returning the phase
function value divided by pdf, also we need to fill in the sampled direction `wo` as bsdf `sample` method.

```cpp
virtual float sample(PhaseFunctionQueryRecord &pRec, const Point2f &sample) const = 0;
```
For detailed phase function, I implemented HenyeyGreenstein in `medium.cpp` following pbrt code:
```cpp
float HenyeyGreenstein::sample(PhaseFunctionQueryRecord &pRec, const Point2f &sample) const {
// Compute $\cos \theta$ for Henyey--Greenstein sample
float cosTheta;
if (std::abs(g) < 1e-3)
    cosTheta = 1 - 2 * sample.x();
else {
    float sqrTerm = (1 - g * g) / (1 + g - 2 * g * sample.x());
    cosTheta = -(1 + g * g - sqrTerm * sqrTerm) / (2 * g);
}

// Compute direction _wo_ for Henyey--Greenstein sample
float sinTheta = std::sqrt(std::max((float)0, 1 - cosTheta * cosTheta));
float phi = 2 * M_PI * sample.y();
Vector3f v1, v2;
coordinateSystem(pRec.wi, v1, v2);
pRec.wo = sphericalDirection(sinTheta, cosTheta, phi, v1, v2, pRec.wi);
pRec.measure = ESolidAngle;
// don't need to consider cosine term for phase function
//    return PhaseHG(cosTheta, g) / pdf(pRec);
// cancel out, directly return 1
return 1.f;
}
```
## Interaction
I create a subclass of `intersection`, the `interaction` will only exit inside a volume for in-scattering integration. After
sampled out the next event point along the ray, we need to do the integration at the point like bsdf at surface. Compared to
normal `intersection` class, `interaction` has phase function attached, which could be used to distinguish. For distance sampling,
if t is bigger than $t_max$, then no phase function attached, we wiil do bsdf integration like outside the volumes. We use `isValid` to
detect this situation.
```cpp
struct Interaction: public Intersection {

    /// Direction of the ray
    Vector3f d;

    // phase function of the participating media
    const PhaseFunction *phase;

    Interaction() : Intersection(), phase(nullptr) { }

    ...

    bool isValid() const { return phase != nullptr; }
```

## Null Bsdf
For ray go through the boundary of shape without detected as hitting a shape and stop, I set null bsdf to the shape.
It's essentially a pretty easy discrete measure bsdf:
```cpp
class Null : public BSDF {
public:
    Null(const PropertyList &propList) {
    // no properties
    }

    virtual Color3f eval(const BSDFQueryRecord &bRec) const override {
    /* Discrete BRDFs always evaluate to zero in Nori */
        return Color3f(0.0f);
    }

    virtual float pdf(const BSDFQueryRecord &bRec) const override {
    /* Discrete BRDFs always evaluate to zero in Nori */
        return 0.0f;
    }

    virtual Color3f sample(BSDFQueryRecord &bRec, const Point2f &sample) const override {
        bRec.wo = -bRec.wi;
        bRec.measure = EDiscrete;

        return Color3f(1.f);
    }
    ...
}
```

## Volumetric Path Tracer
I implemented volumetric path tracer for mats and mis both following normal path tracer, they are very similar except we need to
consider one more situations where the ray is inside the volumes. For mats volumetric path tracer, I marked the difference compared to normal `path_mats`:
```cpp
// path reuse version and optimized loop
Color3f Li(const Scene *scene, Sampler *sampler, const Ray3f &ray) const {
    ... handling the env map if not intersect with scene ...

    while (bounces < m_max_depth) {

        // update the length of ray
        float length = (its.p - shadowRay.o).norm();
        shadowRay = Ray3f(shadowRay, Epsilon, length - Epsilon);

        // intersection with emitter
        // direct illumination
        if (its.mesh->isEmitter()) {
            ... emitter radiacne eval preparation ...
        if (shadowRay.medium) {
            // account Tr for attenuation if inside a medium
            Lo += t * shadowRay.medium->Tr(shadowRay) * Li;
        } else {
            // not inside a medium, no attenuation
            Lo += t * Li;
        }
        ... russian roulette ...

        bool sampleMedium = false;
        // sigma_s * Tr / pdf_t
        Color3f distanceValue(1.f);
        // medium interaction point
        Interaction ita;
        if (shadowRay.medium) {
            // ray associated with medium
            // sample t to decide how far to go
            distanceValue = shadowRay.medium->sample(shadowRay, sampler, ita);
            // still inside the volume not touching the boundary
            sampleMedium = ita.isValid();
        }

        if (sampleMedium) {
            // scattering inside a medium
            // wi for pRec
            PhaseFunctionQueryRecord pRec(-shadowRay.d);
            // sample wo for pRec
            float phaseValue = ita.phase->sample(pRec, sampler->next2D());

            // update throughout for NEE
            t *= distanceValue * phaseValue;

            // new shadowRay
            // still use the old medium
            shadowRay = Ray3f(ita.p, pRec.wo, shadowRay.medium);
        } else {
            ... sampling a new ray using bsdf ...

            // new shadowRay
            Vector3f wo = its.shFrame.toWorld(bRec.wo);
            shadowRay = Ray3f(its.p, wo);

            if (wo.dot(its.shFrame.n) < 0.f) {
                // goes into a shape, use associated medium
                shadowRay.medium = its.mesh->getMedium();
            } else {
                // goes into a shape, use associated medium
                shadowRay.medium = nullptr;
            }
        }
    bounces++;
    }
    return Lo;
}
```

For mis, I keep everything the same inside the volume (so no emitter sampling inside the volume), and do mis outside the volume,
it's almost the same as above, except considering weights of contribution outside and do emitter sampling also:
```cpp
// path reuse version and optimized loop
Color3f Li(const Scene *scene, Sampler *sampler, const Ray3f &ray) const {
    ... handling the env map if not intersect with scene ...

    while (bounces < m_max_depth) {
        // update the length of ray
        float length = (its.p - shadowRay.o).norm();
        shadowRay = Ray3f(shadowRay, Epsilon, length - Epsilon);

        // intersection with emitter
        // direct illumination
        if (its.mesh->isEmitter()) {
            ... emitter radiacne eval preparation ...
        if (shadowRay.medium) {
            // account Tr for attenuation if inside a medium
            Lo += t * shadowRay.medium->Tr(shadowRay) * Li;
        } else {
            ... calculate weights of material sampling contribution ...
            // not inside a medium, no attenuation
            Lo += t * wMats * Li;
        }
        ... russian roulette ...

        bool sampleMedium = false;
        // sigma_s * Tr / pdf_t
        Color3f distanceValue(1.f);
        // medium interaction point
        Interaction ita;
        if (shadowRay.medium) {
            // ray associated with medium
            // sample t to decide how far to go
            distanceValue = shadowRay.medium->sample(shadowRay, sampler, ita);
            // still inside the volume not touching the boundary
            sampleMedium = ita.isValid();
        }

        if (sampleMedium) {
            // scattering inside a medium
            // wi for pRec
            PhaseFunctionQueryRecord pRec(-shadowRay.d);
            // sample wo for pRec
            float phaseValue = ita.phase->sample(pRec, sampler->next2D());

            // update throughout for NEE
            t *= distanceValue * phaseValue;

            // new shadowRay
            // still use the old medium
            shadowRay = Ray3f(ita.p, pRec.wo, shadowRay.medium);
        } else {
            ... Emitter sampling for contribution ...

            ... sampling a new ray using bsdf ...

            // new shadowRay
            Vector3f wo = its.shFrame.toWorld(bRec.wo);
            shadowRay = Ray3f(its.p, wo);

            if (wo.dot(its.shFrame.n) < 0.f) {
                // goes into a shape, use associated medium
                shadowRay.medium = its.mesh->getMedium();
            } else {
                // goes into a shape, use associated medium
                shadowRay.medium = nullptr;
            }
        }
    bounces++;
    }
    return Lo;
}
```

## Validation
I compared my results with Mitsuba, because I didn't do emitter sampling when inside the volume, so turn down the switch in mitsuba as well. And the input paramters should be
transformed a little bit between nori and Mitsuba (using albedo).

First I test a low extinction coefficient:
```xml
<medium type="homogeneous">
    <color name="sigma_a" value="0.99 0.9 0.96"/>
    <color name="sigma_s" value="4.01 4.1 4.04"/>

    <phase type="hg">
        <float name="g" value="0.7"/>
    </phase>
    <boolean name="sample_emitters" value="false" />
</medium>
```

<div class="twentytwenty-container">
    <img src="ref/medium/cbox_sphere_homogeneous_mitsuba.png" alt="Mitsuba, white homogeneous" class="img-responsive">
    <img src="solution/medium/cbox_sphere_homogeneous_mats_nori.png" alt="Mine, mats white homogeneous" class="img-responsive">
    <img src="solution/medium/cbox_sphere_homogeneous_mis_nori.png" alt="Mine, mis white homogeneous" class="img-responsive">
</div>

For mis it's the same with Mitsuba, for mats it gets more noise with expectation.

I also changed the scattering spectrum to get different color inside the volume:
```xml
<medium type="homogeneous">
    <color name="sigma_a" value="5 0.5 5"/>
    <color name="sigma_s" value="0 4.5 0"/>
    <phase type="hg">
        <float name="g" value="0.7"/>
    </phase>
    <boolean name="sample_emitters" value="false" />
</medium>
```

<div class="twentytwenty-container">
    <img src="ref/medium/cbox_sphere_homogeneous_green_mitsuba.png" alt="Mitsuba, green homogeneous" class="img-responsive">
    <img src="solution/medium/cbox_sphere_homogeneous_mats_green_nori.png" alt="Mine, mats green homogeneous" class="img-responsive">
    <img src="solution/medium/cbox_sphere_homogeneous_mis_green_nori.png" alt="Mine, mis green homogeneous" class="img-responsive">
</div>

I also make the extinction coefficient higher (dense volumes):
```xml
<medium type="homogeneous">
    <color name="sigma_a" value="5 0.5 5"/>
    <color name="sigma_s" value="0 4.5 0"/>
    <phase type="hg">
        <float name="g" value="0.7"/>
    </phase>
    <boolean name="sample_emitters" value="false" />
</medium>
```

<div class="twentytwenty-container">
    <img src="ref/medium/cbox_sphere_homogeneous_dense_mitsuba.png" alt="Mitsuba, dense green homogeneous" class="img-responsive">
    <img src="solution/medium/cbox_sphere_homogeneous_mis_dense_nori.png" alt="Mine, dense green homogeneous" class="img-responsive">
</div>

At last I tested the hg phase function with different g including forward, backward, and isotropic medium:
```xml
<medium type="homogeneous">
    <color name="sigma_a" value="5 0.5 5"/>
    <color name="sigma_s" value="0 4.5 0"/>
    <phase type="hg">
        <float name="g" value="0.7"/>
    </phase>
    <boolean name="sample_emitters" value="false" />
</medium>
```

```xml
<medium type="homogeneous">
    <color name="sigma_a" value="5 0.5 5"/>
    <color name="sigma_s" value="0 4.5 0"/>
    <phase type="hg">
        <float name="g" value="0"/>
    </phase>
    <boolean name="sample_emitters" value="false" />
</medium>
```

```xml
<medium type="homogeneous">
    <color name="sigma_a" value="5 0.5 5"/>
    <color name="sigma_s" value="0 4.5 0"/>
    <phase type="hg">
        <float name="g" value="-0.7"/>
    </phase>
    <boolean name="sample_emitters" value="false" />
</medium>
```

<div class="twentytwenty-container">
    <img src="ref/medium/cbox_sphere_homogeneous_mitsuba.png" alt="Mitsuba, forward homogeneous" class="img-responsive">
    <img src="solution/medium/cbox_sphere_homogeneous_mis_nori.png" alt="Mine, forward homogeneous" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="ref/medium/cbox_sphere_homogeneous_isotropic_mitsuba.png" alt="Mitsuba, isotropic homogeneous" class="img-responsive">
    <img src="solution/medium/cbox_sphere_homogeneous_mis_isotropic_nori.png" alt="Mine, isotropic homogeneous" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="ref/medium/cbox_sphere_homogeneous_backward_mitsuba.png" alt="Mitsuba, backward homogeneous" class="img-responsive">
    <img src="solution/medium/cbox_sphere_homogeneous_mis_backward_nori.png" alt="Mine, backward homogeneous" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="solution/medium/cbox_sphere_homogeneous_mis_nori.png" alt="Mine, forward homogeneous" class="img-responsive">
    <img src="solution/medium/cbox_sphere_homogeneous_mis_isotropic_nori.png" alt="Mine, isotropic homogeneous" class="img-responsive">
    <img src="solution/medium/cbox_sphere_homogeneous_mis_backward_nori.png" alt="Mine, backward homogeneous" class="img-responsive">
</div>

We could find for isotropic, the upper part is lighter compared to the bottom part due to extinction. For forward,
the both upper and bottom part is lighter. And for backward, both parts become darker. It meets with expectation.

<h1 id="Disney BSDF">Disney BSDF</h1>

**modified files:**

- `src/disney/disney.cpp`

- `src/warp.cpp`

- `include/nori/warp.h`

- `src/warptest.cpp`

- `include/nori/texture.h`

- `src/map/texture_map.cpp`


This feature permits us using lots of parameters to simulate the materials that we want like metal, plastic, etc. Without necessarily obeying the physical rules,
disney brdf allow us tuning these parameters to change the materials with intuition. My graded parameters are roughness, subsurface, metallic, specular, clearcoat.

As far as I know, there are many implementation methods for disney brdf even with an enhanced version (with scattering)
for the original <a href="https://media.disneyanimation.com/uploads/production/publication_asset/48/asset/s2012_pbs_disney_brdf_notes_v3.pdf" target="_blank">paper</a>.
For theory, I mainly follow the paper. For implementation, I first tried the <a href="https://cseweb.ucsd.edu/~tzli/cse272/wi2023/homework1.pdf" target="_blank"> method from UCSD </a>.
But due to the detailed implementation (formula), the results varied a lot with Mitsuba. Then I turned to follow this paper for GTR1 and GTR2 sampling and pdf,
and the <a href="https://github.com/wdas/brdf/blob/main/src/brdfs/disney.brdf" target="_blank">disney studio implementation</a>
for brdf eval with a more close results.

In summary, there should be an eval function for evaluate the value for each component including diffuse, subsurface, specular, clearcoat.
And lobes for sampling and pdf evaluation including cosine weighted hemisphere, GTR2 and GTR1, which will be used for importance sampling. For my final scene, I should make
the renderer could load roughness and metallic map as input. So there's a small fix for relative parameters calculation (queried value from map not limited to fix ones).

## eval

I followed the disney studio implementation for eval part.
``` cpp
virtual Color3f eval(const BSDFQueryRecord &bRec) const override {
    if (bRec.measure != ESolidAngle
    || Frame::cosTheta(bRec.wi) <= 0
    || Frame::cosTheta(bRec.wo) <= 0)
    return Color3f(0.0f);

    float cosTheta_i = Frame::cosTheta(bRec.wi);
    float cosTheta_o = Frame::cosTheta(bRec.wo);
    Normal3f wh = (bRec.wi + bRec.wo).normalized();
    Color3f baseColor = m_albedo->eval(bRec.uv);
    Color3f Ctint(1.f);
    if (baseColor.getLuminance() > 0) {
    Ctint = baseColor / baseColor.getLuminance();
    }
    Color3f Cspec0 = getMetallic(bRec) * baseColor + (1 - getMetallic(bRec)) * m_specular * .08 * (m_specularTint * Ctint + (1 - m_specularTint));
    Color3f Csheen = m_sheenTint * Ctint + (1 - m_sheenTint);

    // diffuse component
    float wh_dot_wo = wh.dot(bRec.wo);
    float FO = SchlickFresnel(cosTheta_o); // FL
    float FI = SchlickFresnel(cosTheta_i); // FV
    float Fd90 = 0.5f + 2 * getRoughness(bRec) * wh_dot_wo * wh_dot_wo;
    float Fd = lerp(FO, 1.f, Fd90) * lerp(FI, 1.f, Fd90);

    // subsurface component
    float Fss90 = getRoughness(bRec) * wh_dot_wo * wh_dot_wo;
    float Fss = lerp(FO, 1.f, Fss90) * lerp(FI, 1.f, Fss90);
    float ss = 1.25 * (Fss * (1.f / (cosTheta_i + cosTheta_o) - 0.5) + 0.5);

    // specular component
    // include cosTheta_h already, need to cancel out
    float Ds = Warp::squareToGTR2Pdf(wh, get_alpha_x(bRec), get_alpha_y(bRec)) / Frame::cosTheta(wh);
    float FH = SchlickFresnel(wh_dot_wo);
    Color3f Fs = FH + (1 - FH) * Cspec0;
    //        float Gs = smithG_GGX_aniso(bRec.wi, m_alpha_x, m_alpha_y) * smithG_GGX_aniso(bRec.wo, m_alpha_x, m_alpha_y); // light ring outside
    float Gs = G_m(bRec);
    //        Color3f Fsheen = FH * m_sheen * Csheen;

    // clearcoat component
    // include cosTheta_h already, need to cancel out
    float Dr = Warp::squareToGTR1Pdf(wh, m_alpha_g) / Frame::cosTheta(wh);
    float Fr = lerp(FH, 0.04, 1.0);
    //        float Gr = smith_GGX(bRec.wi, 0.25) * smith_GGX(bRec.wo, 0.25);// light ring outside
    float Gr = G_c(bRec.wi, bRec.wo);

    float diffuseWeight = 1 - getMetallic(bRec);
    float specularWeight = 1;
    float clearcoatWeight = 0.25 * m_clearcoat;
    // normalize like microfacet
    return (INV_PI * lerp(m_subsurface, Fd, ss) * baseColor) * diffuseWeight
    + (Gs * Fs * Ds) / (4.f * cosTheta_o) + clearcoatWeight * Gr * Fr * Dr / (4.f * cosTheta_o);
}
```

What need to be careful is we sample wo for diffuse part, and wh for specular and clearcoat part. So there should be a jacobian for transfering.
Also using the `smithG_GGX_aniso` and `smith_GGX` provided in <a href="https://github.com/wdas/brdf/blob/main/src/brdfs/disney.brdf" target="_blank">disney studio implementation</a>
will provide weired white edges, I changed the two geometry term calculations using <a href="https://cseweb.ucsd.edu/~tzli/cse272/wi2023/homework1.pdf" target="_blank"> method from UCSD </a>.

G_m:
```cpp
float lambda_m(const BSDFQueryRecord &bRec, Vector3f w) const {
    float xSquare = pow(w.x() * get_alpha_x(bRec), 2);
    float ySquare = pow(w.y() * get_alpha_y(bRec), 2);
    return (sqrtf(1 + (xSquare + ySquare) / (w.z() * w.z())) - 1) / 2;
}

float g_m(const BSDFQueryRecord &bRec, Vector3f w) const {
    return 1.0f / (1 + lambda_m(bRec, w));
}

// Metallic geometry term
float G_m(const BSDFQueryRecord &bRec) const {
    return g_m(bRec, bRec.wi) * g_m(bRec, bRec.wo);
}

```
G_c:
```cpp
float lambda_c(Vector3f w) const {
    float xSquare = w.x() * 0.25 * w.x() * 0.25;
    float ySquare = w.y() * 0.25 * w.y() * 0.25;
    return (sqrtf(1 + ((xSquare + ySquare) / (w.z() * w.z()))) - 1) / 2;
}

float g_c(Vector3f w) const {
    return 1.0f / (1 + lambda_c(w));
}

// Clearcoat geometry term
float G_c(Vector3f wi, Vector3f wo) const {
    return g_c(wi) * g_c(wo);
}

```

## sampling and pdf

Apart from cosine weighted hemisphere sampling, we should add GTR1 and GTR2 sampling and pdf evaluation. I followed the 2012 paper's appendix.

GTR1 sampling:
```cpp
Vector3f Warp::squareToGTR1(const Point2f &sample, float alpha) {
    //sampling a half vector wh
    float phi = 2 * M_PI * sample.x();

    //following the Disney bsdf paper
    float cosTheta2 = (1 - pow(alpha, 2 - 2 * sample.y())) / (1 - alpha * alpha);
    cosTheta2 = std::clamp(cosTheta2, 0.0f, 1.0f);

    float cosTheta = sqrtf(cosTheta2);
    float sinTheta = sqrtf(1 - cosTheta2);
    Vector3f wh;
    wh.x() = sinTheta * cos(phi);
    wh.y() = sinTheta * sin(phi);
    wh.z() = cosTheta;
    wh.normalize();

    return wh;
}
```
GTR1 pdf:

```
cpp
float Warp::squareToGTR1Pdf(const Vector3f &m, float alpha) {
    // m is the half vector wh, already in local frame
    float cosTheta = Frame::cosTheta(m);
    if (cosTheta < 0) {
        return 0.0f;
    }
    float alpha_2 = alpha * alpha;
    return cosTheta * (alpha_2 - 1) * INV_TWOPI / (log(alpha)) / (1 + (alpha_2 - 1) * cosTheta * cosTheta);
}
```

GTR2 sampling:
```cpp
Vector3f Warp::squareToGTR2(const Point2f &sample, float alpha_x, float alpha_y) {
    //sampling a half vector wh
    float phi = 2 * M_PI * sample.x();

    //following the Disney bsdf paper
    float sinPhi = alpha_y * sin(phi);
    float cosPhi = alpha_x * cos(phi);
    float thanTheta = sqrtf(sample.y() / (1 - sample.y() + SEpsilon));
    // tangent and bitangent and normal in local frame
    Vector3f wh = thanTheta * (cosPhi * Vector3f(1.0f, 0, 0) + sinPhi * Vector3f(0, 1.0f, 0)) + Vector3f(0, 0, 1.0f);
    wh.normalize();

    return wh;
}
```

GTR2 pdf:
```cpp
float Warp::squareToGTR2Pdf(const Vector3f &m, float alpha_x, float alpha_y) {
// m is the half vector wh, already in local frame
    float cosTheta = Frame::cosTheta(m);
    if (cosTheta < 0) {
        return 0.0f;
    }
    float x_normalized = (m.x() / alpha_x) * (m.x() / alpha_x);
    float y_normalized = (m.y() / alpha_y) * (m.y() / alpha_y);
    float z = m.z() * m.z();
    return cosTheta * INV_PI / (alpha_x * alpha_y) / (x_normalized + y_normalized + z) / (x_normalized + y_normalized + z);
}

```
Together pdf sampling:

```cpp
virtual Color3f sample(BSDFQueryRecord &bRec, const Point2f &_sample) const override {
    bRec.measure = ESolidAngle;

    float diffuseWeight = 1 - getMetallic(bRec);
    float specularWeight = 1;
    float clearcoatWeight = 0.25 * m_clearcoat;
    float total = diffuseWeight + specularWeight + clearcoatWeight;

    Vector3f lobePdf(diffuseWeight / total, specularWeight / total, clearcoatWeight / total);
    // importance sampling
    // need to fill in the bRec.wo in each sample component
    // need to sample reuse(renomalize)
    // specular part
    if (_sample.x() < lobePdf[1] + lobePdf[2]) {
        Point2f sample(_sample);
        sample.x() /= lobePdf[1] + lobePdf[2];
        float pdf_clearcoat = lobePdf[2] / (lobePdf[1] + lobePdf[2]);
        // clearcoat component
        if (sample.x() < pdf_clearcoat) {
            sample.x() /= pdf_clearcoat;
            sampleClearcoat(bRec, sample);
        } else {
            // metal component
            sample.x() = (sample.x() - pdf_clearcoat) / (1.f - pdf_clearcoat);
            sampleMetal(bRec, sample);
        }
    } else {
        // diffuse part
        Point2f sample(_sample);
        sample.x() = (sample.x() - (lobePdf[1] + lobePdf[2])) / (1.f - lobePdf[1] + lobePdf[2]);
        sampleDiffuse(bRec, sample);
    }

    float pdfValue = pdf(bRec);
    if (pdfValue < SEpsilon) {
        return Color3f(0.0f);
    }
    return eval(bRec) * Frame::cosTheta(bRec.wo) / pdfValue;
}


```

Together pdf
```cpp
virtual float pdf(const BSDFQueryRecord &bRec) const override {
    if (bRec.measure != ESolidAngle
    || Frame::cosTheta(bRec.wi) <= 0
    || Frame::cosTheta(bRec.wo) <= 0)
        return 0.0f;

    float diffuseWeight = 1 - getMetallic(bRec);
    float specularWeight = 1;
    float clearcoatWeight = 0.25 * m_clearcoat;
    float total = diffuseWeight + specularWeight + clearcoatWeight;
    Vector3f weight(diffuseWeight / total, specularWeight / total, clearcoatWeight / total);
    // blending pdf should in [0, 1]
    return weight[0] * pdfDiffuse(bRec) + weight[1] * pdfSpecular(bRec) + weight[2] * pdfClearcoat(bRec);
}
```


## Validation
First we have to make newly added sampling lobes work via warptest:

<div class="twentytwenty-container">
    <img src="solution/disney/GTR1.png" alt="GTR1" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="solution/disney/GTR1_test.png" alt="GTR1 test" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="solution/disney/GTR2.png" alt="GTR2" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="solution/disney/GTR2_test.png" alt="GTR2 test" class="img-responsive">
</div>

Second, we have to validate in real scenes:

For roughness, subsurface, metallic, specular, clearcoat, these 5 parameters, I changed the value from 0 to 1 with 0.1 pace while keeping other parameters as default values,
and compared the results with Mitsuba. All the scenes are lit by a single point sphere and the same hight res env map.

Because the implementation details varied a lot, especially the formula. And the Mitsuba implementation even included the <a href="https://media.disneyanimation.com/uploads/production/publication_asset/48/asset/s2012_pbs_disney_brdf_notes_v3.pdf" target="_blank">enhanced version of disney bsdf</a>,
like refraction which is not covered by the 2012's version. Then it could handle the situation where the rays are inside the object, but I simply return 0 if inside (no transmission at all).
They calculate the fresnel coefficient using eta to get `F`, I just followed the implementation to approximate the `F_m` and `F_c` using specular parameter. So there might be a mismatch as well.

Besides, the light in Mitsuba is somehow lighter compared to Nori like previous mentioned (texture and normal comparison). So my results will be darker than Mitsuba, but that's not correlated with disney brdf feature, so please ignore them.

For roughness, I could find the material indeed varied from smooth ones to rough ones. But the highlight is bigger than Mitsuba especially for small roughness parameters.
Mitsuba considered the fresnel coefficients for specular part, and retro-reflection for diffuse part, which is more complex and advanced.
<div class="twentytwenty-container">
    <img src="ref/disney/disney_roughness_mitsuba.png" alt="Mitsuba, roughness" class="img-responsive">
    <img src="solution/disney/disney_roughness_nori.png" alt="Mine, roughness" class="img-responsive">
</div>

For subsurface, the results look very similar, because the subsurface eval is the same. But due to the retro-reflection, Mitsuba results seems lighter.

<div class="twentytwenty-container">
    <img src="ref/disney/disney_subsurface_mitsuba.png" alt="Mitsuba, subsurface" class="img-responsive">
    <img src="solution/disney/disney_subsurface_nori.png" alt="Mine, subsurface" class="img-responsive">
</div>

For metallic, when it increases, the material is indeed with more metallic sense, also due to the retro-reflection and fresnel coefficient, my edges seem darker.

<div class="twentytwenty-container">
    <img src="ref/disney/disney_metallic_mitsuba.png" alt="Mitsuba, metallic" class="img-responsive">
    <img src="solution/disney/disney_metallic_nori.png" alt="Mine, metallic" class="img-responsive">
</div>

For specular, results look very similar, when it increases, the highlight indeed becomes more apparent.
<div class="twentytwenty-container">
    <img src="ref/disney/disney_specular_mitsuba.png" alt="Mitsuba, specular" class="img-responsive">
    <img src="solution/disney/disney_specular_nori.png" alt="Mine, specular" class="img-responsive">
</div>

For clearcoat, my highlight seems darker than Mitsuba. Mitsuba also considered inside situation according to eta, but I just return 0, if the rays are inside.

<div class="twentytwenty-container">
    <img src="ref/disney/disney_clearcoat_mitsuba.png" alt="Mitsuba, clearcoat" class="img-responsive">
    <img src="solution/disney/disney_clearcoat_nori.png" alt="Mine, clearcoat" class="img-responsive">
</div>

I also tested on the final scene:
<div class="twentytwenty-container">
    <img src="solution/final/final.png" alt="Disney Brdf Test" class="img-responsive">
</div>

<!-- Bootstrap core CSS and JavaScript -->

<link href="../resources/offcanvas.css" rel="stylesheet">
<link href="../resources/twentytwenty.css" rel="stylesheet" type="text/css" />

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="../resources/bootstrap.min.js"></script>
<script src="../resources/jquery.event.move.js"></script>
<script src="../resources/jquery.twentytwenty.fix.js"></script>

<!-- Markdeep: -->
<script>var markdeepOptions = {onLoad: function() {$(".twentytwenty-container").twentytwenty({default_offset_pct: 0.5, move_slider_on_hover: true, placeholder: '../resources/nori.png'});},tocStyle:'none'};</script>
<script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>
<script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>