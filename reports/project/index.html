<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">

**Report**
Comparison pipeline:
1. Establish scene in blender.
2. Export Nori.xml and Mitsuba.xml respectively
3. Check the xml files with necessary manual adjustment
4. Align the ply and obj file, ensure they are the same
5. Rendering with both ones and compare the images

# Images as Textures
**modified files:**

- `src/map/texture_map.cpp`


Given an image, we can attach the image onto object, making it looks more realistic without increasing the geometry complexity of mesh.
Specifically, for arbitrary point in 3D space, we could map it into a 2D space point, and query the color of that 2D point from the given image.
The most important thing is to understand the mapping from UV space to XY image space, because UV space is continuous but XY space is discrete (only integer indices has pixel value).
$$
    x = \frac{u}{\frac{1}{width}}=u \times width \\
    y = \frac{v}{\frac{1}{height}}=u \times height
$$

To make XY space become continuous, we could use bilinear interpolation with the values in the four corners, like:
```cpp
// integer part
int x_int = int(std::floor(x));
int y_int = int(std::floor(y));

float x_frac = x - float(x_int);
float y_frac = y - float(y_int);

T interpolated_value =
    (1 - x_frac) * (1 - y_frac) * v00 +
    x_frac * (1 - y_frac) * v10 +
    (1 - x_frac) * y_frac * v01 +
    x_frac * y_frac * v11;
```

Also need to consider the normalization of given uv input (maybe negative, maybe out of range), and need to align the frame
convention (e.g., origin, axis) with the output format of Blender using the checkerboard pattern image on a plane:
```cpp
// map UV in any range to [0, 1]
Point2f normalized_uv;
normalized_uv.x() = uv.x() - std::floor(uv.x());
normalized_uv.y() = uv.y() - std::floor(uv.y());
// flip V to align convention
// UV coord set origin at left bottom
// image coord set origin at left top
normalized_uv.y() = 1.0f - normalized_uv.y();

//change to image space
// swap x and y
float y = normalized_uv.x() / m_scale.x() * (m_width - 1);
float x = normalized_uv.y() / m_scale.y() * (m_height - 1);
```
<div class="twentytwenty-container">
    <img src="ref/texture/floor_mitsuba.png" alt="Mitsuba" class="img-responsive">
    <img src="solution/texture/floor_nori.png" alt="Mine" class="img-responsive">
</div>

The scale property could make the image applied on mesh after scaling, like using`(0.5, 1)` would make u direction to densify as 2 times of the original one:
<div class="twentytwenty-container">
    <img src="solution/texture/floor_scale_nori.png" alt="(0.5,1)" class="img-responsive">
    <img src="solution/texture/floor_nori.png" alt="(1,1)" class="img-responsive">
</div>

This feature support `jpg`, `png` and `exr` formats. For `jpg` and `png` formats, I use `stbi` tiny library to process the data, careful with the layout (RGBA, row major).

```cpp
std::cout << "Processing jpg/png file: " << filename << std::endl;
// using stbi library
// 1.RGBA/RGB 2.column first order lay out
int channels;
unsigned char* data = stbi_load(filename.c_str(), &m_width, &m_height, &channels, 3);
if (!data) {
    std::cerr << "Failed to load jpg/png image: " << filename << std::endl;
    return false;
}
m_normal.reserve(m_width * m_height);
for (int x = 0; x < m_width; ++x) {
    for (int y = 0; y < m_height; ++y) {
        //careful with channels!
        float R = data[channels * (y * m_width + x)] / 255.0f;
        float G = data[channels * (y * m_width + x) + 1] / 255.0f;
        float B = data[channels * (y * m_width + x) + 2] / 255.0f;
        m_normal.push_back(Color3f(R, G, B));
    }
}
//release mem
stbi_image_free(data);
```

For `exr` format, I use `bitmap` which is already implemented.
```cpp
std::cout << "Processing EXR file: " << filename << std::endl;
// using bit map
Bitmap bitmap(filename);
m_height = bitmap.rows();
m_width = bitmap.cols();
for (int x = 0; x < m_width; ++x) {
    for (int y = 0; y < m_height; ++y) {
        m_normal.push_back(bitmap(y, x));
    }
}
```
What need to be careful about is the notion of `sRGB` and `linearRGB`, when we do calculating (e.g., integration)
we use `linearRGB`, but the color of image is stored as `sRGB`, so a transformation (gamma correction) after reading the image is necessary.
<div class="twentytwenty-container">
    <img src="solution/texture/sphere_nori.png" alt="toLinear" class="img-responsive">
    <img src="solution/texture/sphere_sRGB_nori.png" alt="sRGB" class="img-responsive">
</div>


## Validation
I used the plugin of Blender to export xml format of Mitsuba and Nori respectively. Then I used Mitsuba and Nori to render the same scene, using path tracing with mis (default parameters, spp is 32) and diffuse bsdf.
<div class="twentytwenty-container">
    <img src="ref/texture/sphere_mitsuba.png" alt="Mitsuba" class="img-responsive">
    <img src="solution/texture/sphere_nori.png" alt="Mine" class="img-responsive">
</div>

We could find the results are almost the same. The minor shadow difference is due to the implementation of path tracer. I suppose it's some different parameters like `max_depth` causing this.

# Normal Mapping
**modified files:**

- `include/nori/bsdf.h`

- `include/nori/mesh.h`

- `src/map/normal_map.cpp`

- `src/diffuse.cpp`

- `src/obj.cpp`

- `src/mesh.cpp`




Like image texture, we could use normal mapping to render more realistic pictures without modifying the real detailed geometry
(just adjusting the shading normal of the hitting point using the queried normal). Almost the same as image texture
(e.g., repeat wrap mode, bilinear interpolation,  map scaling), we could implement the normal version of image texture. What important is the range of normal, we should first
transform the rgb range (maybe out of range for EXR image) into `[0, 1]` using
$$ v' = \frac{v - min(img)}{max(img) - min(img)}$$
then transfer the range from `[0, 1]` into `[-1, 1]`:
$$ n = 2 \times v' - 1$$

```cpp
Color3f normalized_rgb = (rgb - min) / (max - min);
```

And also because the normal should be stored into the normal map directly (without gamma correction), so we don't need to map the value back as well when using:
```cpp
    T getPixel(int x, int y) {
        return m_normal[y * m_width + x];
    }
```
First change bsdf.h interface to allow us query the normal:

```cpp
virtual bool getNormalMap(Texture<Normal3f> * &normal_map) const { return false; }
```

For diffuse.cpp, add normalmap object as child:
```cpp
    if (obj->getIdName() == "normalmap") {
        if (m_normalmap) {
            throw NoriException("There is already a normalmap defined!");
        }
        m_normalmap = static_cast<Texture<Normal3f> *>(obj);
    }
```

To equip with consistent tangent across the surface, we could use UV coords to compute a shading frame instead of just using geometry frame (not consistent in UV space),
        we compute the TBN matrix when loading object in `obj.cpp`:
```cpp
    void compute_pervertex_TBN() {
        // column first for Eigen
        // Initialize m_T and m_B to have the same size as m_V (3, N)
        m_T.resize(3, m_V.cols());
        m_B.resize(3, m_V.cols());

        // Iterate over each face to compute tangents and bitangents
        for (int i = 0; i < m_F.cols(); ++i) {
            // Get vertex indices for the current face
            int idx0 = m_F(0, i);
            int idx1 = m_F(1, i);
            int idx2 = m_F(2, i);

            // Get positions and UVs for the current face
            Vector3f p0 = m_V.col(idx0);
            Vector3f p1 = m_V.col(idx1);
            Vector3f p2 = m_V.col(idx2);

            Vector2f uv0 = m_UV.col(idx0);
            Vector2f uv1 = m_UV.col(idx1);
            Vector2f uv2 = m_UV.col(idx2);

            // Compute edges and delta UVs
            Vector3f edge1 = p1 - p0;
            Vector3f edge2 = p2 - p0;
            Vector2f deltaUV1 = uv1 - uv0;
            Vector2f deltaUV2 = uv2 - uv0;

            // Compute tangent and bitangent for this face
            float r = 1.0f / (deltaUV1.x() * deltaUV2.y() - deltaUV1.y() * deltaUV2.x());
            Vector3f T = r * (deltaUV2.y() * edge1 - deltaUV1.y() * edge2);
            Vector3f B = r * (-deltaUV2.x() * edge1 + deltaUV1.x() * edge2);

            // Accumulate the results to the vertices
            m_T.col(idx0) += T;
            m_T.col(idx1) += T;
            m_T.col(idx2) += T;

            m_B.col(idx0) += B;
            m_B.col(idx1) += B;
            m_B.col(idx2) += B;
        }

        // Normalize accumulated tangents and bitangents
        for (int i = 0; i < m_V.cols(); ++i) {
            m_T.col(i).normalize();
            m_B.col(i).normalize();
        }

        for (int i = 0; i < m_V.cols(); ++i) {
            Vector3f N = m_N.col(i).normalized(); // Per-vertex normal
            Vector3f T = m_T.col(i);
            Vector3f B = m_B.col(i);

            // Re-orthogonalize tangent and compute corrected bitangent
            T = (T - N.dot(T) * N).normalized();
            B = N.cross(T).normalized();

            // Store back the orthogonalized results
            m_T.col(i) = T;
            m_B.col(i) = B;
        }
    }
```
In `mesh.cpp`, at the hit points we need to adjust the normal, and now we have TBN matrix:

```cpp
    if (m_N.size() > 0) {
        Vector3f t = (bary.x() * m_T.col(idx0) +
        bary.y() * m_T.col(idx1) +
        bary.z() * m_T.col(idx2)).normalized();

        Vector3f b = (bary.x() * m_B.col(idx0) +
        bary.y() * m_B.col(idx1) +
        bary.z() * m_B.col(idx2)).normalized();

        Vector3f n = (bary.x() * m_N.col(idx0) +
        bary.y() * m_N.col(idx1) +
        bary.z() * m_N.col(idx2)).normalized();

        its.shFrame = Frame(t, b, n);

    } else {
        its.shFrame = its.geoFrame;
    }

    Texture<Normal3f> *normal_map;
    if (its.mesh->getBSDF()->getNormalMap(normal_map)) {
        // using normal map to reformulate shFrame
        Normal3f local_n = normal_map->eval(its.uv).normalized();
        Normal3f world_n = its.shFrame.toWorld(local_n).normalized();
        its.shFrame = Frame(world_n);
        // TBN already under global
        Vector3f s = its.shFrame.s;
        s = (s - world_n.dot(s) * world_n).normalized();
        Vector3f t = world_n.cross(s);
        its.shFrame = Frame(s, t, world_n);
    }
```


## Validation
First, I use a plane on the ground, and place the point light on the left. We could find normal map makes the rendering more realistic even with the same original flat plane.
<div class="twentytwenty-container">
    <img src="solution/normal/floor_wo_normal_nori.png" alt="wo normal map" class="img-responsive">
    <img src="solution/normal/floor_normal_nori.png" alt="w normal map" class="img-responsive">
</div>

To make sure the convention is the same, I also change the place of light to the top, and compared the results with Mitsuba,
            the light instensity is slightly different, please just ignore that (maybe use env map later to validate again TODO):
<div class="twentytwenty-container">
    <img src="ref/normal/floor_normal_mitsuba.png" alt="Mitsuba" class="img-responsive">
    <img src="solution/normal/floor_normal_nori.png" alt="Mine" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="ref/normal/floor_normal_2_mitsuba.png" alt="Mitsuba" class="img-responsive">
    <img src="solution/normal/floor_normal_2_nori.png" alt="Mine" class="img-responsive">
</div>

Then I tested more complicated materials on sphere:

For to using TBN map:
<div class="twentytwenty-container">
    <img src="ref/normal/sphere_normal_mitsuba.png" alt="Mitsuba, w normal map" class="img-responsive">
    <img src="solution/normal/sphere_normal_wo_TBN_nori.png" alt="Mine, w normal map" class="img-responsive">
    <img src="ref/normal/sphere_wo_normal_mitsuba.png" alt="Mitsuba, wo normal map" class="img-responsive">
    <img src="solution/normal/sphere_wo_normal_nori.png" alt="Mine, wo normal map" class="img-responsive">
</div>
After applying the TBN map:
<div class="twentytwenty-container">
    <img src="ref/normal/sphere_normal_mitsuba.png" alt="Mitsuba, w normal map" class="img-responsive">
    <img src="solution/normal/sphere_normal_nori.png" alt="Mine, w normal map" class="img-responsive">
    <img src="ref/normal/sphere_wo_normal_mitsuba.png" alt="Mitsuba, wo normal map" class="img-responsive">
    <img src="solution/normal/sphere_wo_normal_nori.png" alt="Mine, wo normal map" class="img-responsive">
</div>

We could conclude these:
1. For image texture, TBN doesn't influence a lot, due to not transforming the normal from local to global with shading frame.
2. For plane, even without TBN map, normal map could work well with inherent consistent tangent across the surface.
3. For more complex mesh(e.g., sphere), with TBN map, we could get more consistent and realistic reflection.

# Mesh Modelling

# Environment Map

# Homogeneous Participating Media

# Disney BSDF


<!-- Bootstrap core CSS and JavaScript -->

<link href="../resources/offcanvas.css" rel="stylesheet">
<link href="../resources/twentytwenty.css" rel="stylesheet" type="text/css" />

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="../resources/bootstrap.min.js"></script>
<script src="../resources/jquery.event.move.js"></script>
<script src="../resources/jquery.twentytwenty.fix.js"></script>

<!-- Markdeep: -->
<script>var markdeepOptions = {onLoad: function() {$(".twentytwenty-container").twentytwenty({default_offset_pct: 0.5, move_slider_on_hover: true, placeholder: '../resources/nori.png'});},tocStyle:'none'};</script>
<script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>
<script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
